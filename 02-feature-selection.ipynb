{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleção de Features\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [O que é](#o-que-é)\n",
    "2. [Filters](#filters)\n",
    "    1) [Limiar de Variância](#limiar-de-variância)\n",
    "    2) [KBest](#kbest)\n",
    "3. [Intrinsic](#intrinsic)\n",
    "4. [Wrapper](#wrapper)\n",
    "5. [Referências](#referências)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é\n",
    "\n",
    "Seleção de _features_ é o processo em que _features_ são concientemente removidas do dataset para melhorar a qualidade dos dados. Por exemplo, _features_ redundantes podem ser removidas do dataset sem prejuízo. Beyer [1] mostra como uma quantidade maior de _features_ pode ser deletéria para classificação, especificamente com _kNN_. A depender da característica dos datasets, uma grande quatnidade de _features_ pode reduzir a variabilidade da medida da distância euclidiana.\n",
    "\n",
    "Métodos de seleção de _features_ podem ser divididos em:\n",
    "\n",
    "- _filter_: usam métodos estatísticos para seleção das melhores _features_.\n",
    "- _intrinsic_: são métodos de seleção de features que acontecem intrisicamente nos classificadores. Podemos citar uma árvore de decisão como exemplo, caso a altura da árvore seja limitada, as melhores _features_ são escolhidas primeiro. Assim sendo, esse classificador possui uma seleção de features instrísica.\n",
    "- _wrapper_: esses métodos criam vários classificadores, treinados em diferentes _features_, e a _features_ serão escolhidas a partir do melhor classificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar um dataset de classificação. A função _make_classificaiton_ cria um dataset com 100 instancias, 4 features e apenas 2 features contém informção não-redundante propositalmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(n_samples=100, n_features=4, n_informative=2, random_state=199)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=199)\n",
    "df = pd.DataFrame(X, columns=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando cada um dos pares de _features_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas de manipualção e visualização de dados\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# vamos definir cores e simbolos para nossas classses\n",
    "colors = {0: \"steelblue\", 1: \"darkorange\", 2: \"mediumseagreen\"}\n",
    "markers = {0: \"s\", 1: \"^\", 2:\"o\"}\n",
    "\n",
    "# vamos criar uma função para visualizar uma região 2d, dadas duas features\n",
    "def plot_2d_space(X, y, f1, f2, colors=colors, markers=markers):\n",
    "    plt.figure()\n",
    "    labels = list(np.unique(y))\n",
    "    labels.sort()\n",
    "\n",
    "    lines = []\n",
    "    for i in labels:\n",
    "        line = plt.scatter(\n",
    "            X[f1][y==i],\n",
    "            X[f2][y==i], \n",
    "            c=colors[i], \n",
    "            marker=markers[i],\n",
    "            label=i,\n",
    "        )\n",
    "\n",
    "        lines.append(line)\n",
    "\n",
    "    plt.legend(handles=lines)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(i+1, 4):\n",
    "        plot_2d_space(df, y, i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que algumas combinações de features apresentam alguma repetição. Apesar de serem diferentes, o primeiro grafico e o segundo estão espelhados em $y$.\n",
    "\n",
    "Assim, que métodos podemos usar para enfrentar esse problema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters\n",
    "\n",
    "### Limiar de Variância\n",
    "\n",
    "O limiar de variância é o método mais simples de seleção de _features_. Caso a _feature_ não tenha determinada variância, ela deve ser removida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "filter_variance = VarianceThreshold(0.8)\n",
    "X_filtered = filter_variance.fit_transform(X)\n",
    "\n",
    "# quantas features conseguimos remover?\n",
    "print(\"Features selecionadas: %d\" %(X_filtered.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceba que das 4 _features_, nenhuma foi removida, mesmo com o filtro de variância para _features_ com $\\sigma^2 > 0.8$. Esse tipo de método é util quando as _features_ são muito esparsas, como no caso de um dataset com _features_ binárias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KBest\n",
    "\n",
    "Aqui escolhemos a quantidade de _features_ que serão escolhidas após um teste estatístico. O diagrama abaixo serve como base para a escolha do teste disponível no _sklearn_ baseado no tipo de variável de entrada e saída.\n",
    "\n",
    "\n",
    "```\n",
    "                                                    numérica - _r_regression_ (Pearson)\n",
    "                                                   /\n",
    "                      numérica - variável de saída\n",
    "                     /                             \\\n",
    "                    /                                categórica - _f_classif_ (ANOVA)\n",
    "variável de entrada \n",
    "                    \\                                  numérica - _mutual_info_regression_ (Mutual Information)\n",
    "                     \\                               /\n",
    "                       categórica - variável de saída\n",
    "                                                     \\\n",
    "                                                       categórica - _mutual_info_classif_ (Mutual Information)\n",
    "```\n",
    "\n",
    "Cada uma dessas funções mede a correlação entre as features e a variável objetivo. Essas correlações são utilizadas para ranquear as melhores features, ou seja, as features são que são mais correlacionadas com a saída. Após isso, definimos o número de features a serem escolhidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "kbest = SelectKBest(score_func=f_classif, k=2)\n",
    "X_filtered = kbest.fit_transform(X, y)\n",
    "\n",
    "# quantas features conseguimos remover?\n",
    "print(\"Features selecionadas: %d\" %(X_filtered.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizando as features selecionadas\n",
    "df = pd.DataFrame(X_filtered, columns=[0, 1])\n",
    "plot_2d_space(df, y, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intrinsic\n",
    "\n",
    "Vamos relembrar a árvore de decisão. Esse classificador ranqueia as \"melhores\" features para serem utilizadas primeiro, objetivando a melhor separação. Podemos usar esse classificador para selecionar apenas as melhores features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "base_estimator = DecisionTreeClassifier()\n",
    "sfm = SelectFromModel(estimator=base_estimator)\n",
    "X_filtered = sfm.fit_transform(X, y)\n",
    "\n",
    "print(\"Features selecionadas: %d\" %(X_filtered.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apenas uma feature foi selecionada... será que é suficiente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resultado com todas as features\")\n",
    "model = KNeighborsClassifier().fit(X_train, y_train)\n",
    "print(classification_report(y_test, model.predict(X_test)))\n",
    "\n",
    "print(\"\\nResultado com apenas as features selecionadas\")\n",
    "base_estimator = DecisionTreeClassifier()\n",
    "sfm = SelectFromModel(estimator=base_estimator)\n",
    "X_train_filtered = sfm.fit_transform(X_train, y_train)\n",
    "X_test_filtered = sfm.transform(X_test)\n",
    "\n",
    "model = KNeighborsClassifier().fit(X_train_filtered, y_train)\n",
    "print(classification_report(y_test, model.predict(X_test_filtered)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper\n",
    "\n",
    "Aqui, o processo de escolha de features depende de outros classificadores. Escolhemos um classificador linear, onde os coeficientes, ou medida de importância interna ao classificador, são utilizados para escolha das features mais importantes. Esse método é feito de forma recusriva, onde podemos controlar a quantidade de passos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "base_estimator = SVR(kernel=\"linear\")\n",
    "rfe = RFE(base_estimator, n_features_to_select=2, step=1)\n",
    "X_filtered = rfe.fit_transform(X, y)\n",
    "\n",
    "print(X_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizando as features selecionadas\n",
    "df = pd.DataFrame(X_filtered, columns=[0, 1])\n",
    "plot_2d_space(df, y, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "[1] [Beyer, Kevin, et al. \"When is “nearest neighbor” meaningful?.\" International conference on database theory. Springer, Berlin, Heidelberg, 1999.](https://minds.wisconsin.edu/bitstream/handle/1793/60174/TR1377.pdf?sequence=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2740ff0ff5ab647f4caa51cae9634c8f0b586b6f57486f90b97d87d738fcb59"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('mscenv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
